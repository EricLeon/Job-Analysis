# Job Posting Analysis

- Created a custom web crawler to automatically download job posting information and save it to a local SQLite database.


## TO DO / IN PROGRESS:
- EDA -> Mainly text analysis
- Find a source of for industry analysis
- Create a pipeline to clean the data once I figure out what I have to do


## Framing The Problem

- Objective: Identify what skills are most in demand for data analyst jobs in London.


## Data Collection

I created a custom web crawler to collect this data automatically. I chose to scrape the data from Reed as it had the majority of the data points of interest. I initially looked at Glassdoor and LinkedIn but quickly learnt that the data would have been more difficult to extract.

I am scraping pretty much every data point available currently on Reed for each posting. These include:
job_title, posted_by, salary, location, job_type, direct_link, job_description.



## Data Cleaning & Engineering



## Data Visualisation (Tableau)



## Next Steps



## Code & Resources Used

- **Python Version:** 3.7
- **Python Libraries:** Requests, BeautifulSoup, NumPy, pandas, SQLite
- **Tableau Desktop Version:** 
